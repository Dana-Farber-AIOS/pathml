{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14070544-7803-40fb-8f4b-99724b49f224",
   "metadata": {},
   "source": [
    "# PathML Graph construction and processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886bc5f-83db-4abe-97e0-b8bc9b3aab56",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate the ability of the new pathml.graph API to construct cell and tissue graphs. Specifically, we will do the following:\n",
    "\n",
    "1. Use a pre-trained HoVer-Net model to detect cells in a given Region of Interested (ROI)\n",
    "2. Use boundary detection techniques to detect tissues in a given ROI\n",
    "3. Featurize the detected cell and tissue patches using a ResNet model\n",
    "4. Construct both tissue and cell graphs using k-Nearest Neighbour (k-NN) and Region-Adjacency Graph (RAG) methods and save them as torch tensors.\n",
    "\n",
    "To get the full functionality of this notebook for a real-world dataset, we suggest you download the BRACS ROI set from the [BRACS dataset](https://www.bracs.icar.cnr.it/download/). To do so, you will have to sign up and create an account. Next, you will just have to replace the root folder in this tutorial to whatever directory you download the BRACS dataset to. \n",
    "\n",
    "In this notebook, we will use a representative image from this [link](https://github.com/histocartography/hact-net/tree/main/data) stored in `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2399d96-abf7-46b6-b783-c4c292259bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "import h5py\n",
    "import warnings\n",
    "import math\n",
    "from skimage.measure import regionprops, label\n",
    "import networkx as nx\n",
    "import traceback\n",
    "from glob import glob\n",
    "\n",
    "from pathml.core import HESlide, SlideData\n",
    "import matplotlib.pyplot as plt \n",
    "from pathml.preprocessing.transforms import Transform\n",
    "from pathml.core import HESlide\n",
    "from pathml.preprocessing import Pipeline, BoxBlur, TissueDetectionHE, NucleusDetectionHE\n",
    "import pathml.core.tile\n",
    "from pathml.ml import HoVerNet, loss_hovernet, post_process_batch_hovernet\n",
    "\n",
    "from pathml.datasets.utils import DeepPatchFeatureExtractor\n",
    "from pathml.preprocessing import StainNormalizationHE\n",
    "from pathml.graph import RAGGraphBuilder, KNNGraphBuilder\n",
    "from pathml.graph import ColorMergedSuperpixelExtractor\n",
    "from pathml.graph.utils import _valid_image, _exists, plot_graph_on_image, get_full_instance_map, build_assignment_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47ddb6-2edf-42b5-9227-65c9ac1ecbf1",
   "metadata": {},
   "source": [
    "## Building a HoverNetNucleusDetectionHE class using pathml.transforms API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58d8d0-e7a9-4e92-bfd0-2ec1cecafcf6",
   "metadata": {},
   "source": [
    "First, we will use a pre-trained HoVer-Net model to detect cells and return a instance map containing masks that corresponds to cells. We will use the `pathml.preprocessing.transforms` class that can be used to apply a function over each ROI. The new `HoverNetNucleusDetectionHE` simply inherits this class and applies a HoVer-Net model onto each ROI that is passed into it. \n",
    "\n",
    "To obtain the pre-trained HoVer-Net model, we follow the steps in this [tutorial](https://pathml.readthedocs.io/en/latest/examples/link_train_hovernet.html). For simplicity, we provide a pre-trained model under the `pretrained_models` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0358a83-e93a-4525-a7bb-f697e2252ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoverNetNucleusDetectionHE(Transform):\n",
    "    \n",
    "    \"\"\"\n",
    "    Nucleus detection algorithm for H&E stained images using pre-trained HoverNet Model.\n",
    "\n",
    "    Args:\n",
    "        mask_name (str): Name of mask that is created.\n",
    "        model_path (str): Path to the pretrained model. \n",
    "    \n",
    "    References:\n",
    "        Graham, S., Vu, Q.D., Raza, S.E.A., Azam, A., Tsang, Y.W., Kwak, J.T. and Rajpoot, N., 2019. \n",
    "        Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images. \n",
    "        Medical image analysis, 58, p.101563.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mask_name,\n",
    "        model_path = None\n",
    "    ):\n",
    "        self.mask_name = mask_name\n",
    "        \n",
    "        cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "        \n",
    "        if model_path is None:\n",
    "            raise NotImplementedError(\"Downloadable models not available\")\n",
    "        else:\n",
    "            checkpoint = torch.load(model_path)\n",
    "            self.model = HoVerNet(n_classes=6)\n",
    "            self.model.load_state_dict(checkpoint)\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def F(self, image):\n",
    "        assert (\n",
    "            image.dtype == np.uint8\n",
    "        ), f\"Input image dtype {image.dtype} must be np.uint8\"\n",
    "        \n",
    "        image = torch.from_numpy(image).float()\n",
    "        image = image.permute(2, 0, 1)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            out = self.model(image)\n",
    "            preds_detection, _ = post_process_batch_hovernet(out, n_classes=6)\n",
    "        preds_detection = preds_detection.transpose(1,2,0)\n",
    "        return preds_detection\n",
    "\n",
    "    def apply(self, tile):\n",
    "        assert isinstance(\n",
    "            tile, pathml.core.tile.Tile\n",
    "        ), f\"tile is type {type(tile)} but must be pathml.core.tile.Tile\"\n",
    "        assert (\n",
    "            self.mask_name is not None\n",
    "        ), \"mask_name is None. Must supply a valid mask name\"\n",
    "        assert (\n",
    "            tile.slide_type.stain == \"HE\"\n",
    "        ), f\"Tile has slide_type.stain={tile.slide_type.stain}, but must be 'HE'\"\n",
    "        \n",
    "        nucleus_mask = self.F(tile.image)\n",
    "        tile.masks[self.mask_name] = nucleus_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e1bc1-6825-44b6-8909-8d47574295ed",
   "metadata": {},
   "source": [
    "A simple example on using this class is given below. The input image for this is present in the `data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbe1db-17b0-4ddf-8e74-c338bae2c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi = SlideData('../data/example_0_N_0.png', name = 'example', backend = \"openslide\", stain = 'HE')\n",
    "region = wsi.slide.extract_region(location = (900, 800), size = (256, 256))\n",
    "plt.imshow(region)\n",
    "plt.title('Input image', fontsize=11)\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098e3eb-a32f-4d67-88bc-a056fe0207e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_detect = HoverNetNucleusDetectionHE(mask_name = 'cell', model_path = '../pretrained_models/hovernet_fully_trained.pt')\n",
    "cell_mask = nuclei_detect.F(region)\n",
    "plt.imshow(cell_mask)\n",
    "plt.title('Nuclei mask', fontsize=11)\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c3b68-9028-4724-97ad-6d9934e0ce2e",
   "metadata": {},
   "source": [
    "## Cell and Tissue graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b2866-1c88-485c-810b-60f0be998174",
   "metadata": {},
   "source": [
    "Next, we can move on to applying a function that uses the new `pathml.graph` API to construct cell and tissue graphs.\n",
    "\n",
    "We have to first define some constants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9377c10-d38c-4bea-ada9-aaff38cd92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tumor time given in the filename to a label\n",
    "TUMOR_TYPE_TO_LABEL = {\n",
    "    'N': 0,\n",
    "    'PB': 1,\n",
    "    'UDH': 2,\n",
    "    'ADH': 3,\n",
    "    'FEA': 4,\n",
    "    'DCIS': 5,\n",
    "    'IC': 6\n",
    "}\n",
    "\n",
    "# Define minimum and maximum pixels for processing a ROI\n",
    "MIN_NR_PIXELS = 50000\n",
    "MAX_NR_PIXELS = 50000000  \n",
    "\n",
    "# Define the reference image and HoVer-Net model path\n",
    "ref_path = '../data/example_0_N_0.png'\n",
    "hovernet_model_path = '../pretrained_models/hovernet_fully_trained.pt'\n",
    "\n",
    "# Define the patch size for applying HoverNetNucleusDetectionHE \n",
    "PATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5798f-3d93-4179-9bdc-8ae80b38573a",
   "metadata": {},
   "source": [
    "Next, we write the main preprocessing loop as a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa61a77-b882-4161-8fb5-72e57ad2be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(image_path, save_path, split, plot=True, overwrite=False):\n",
    "    # 1. get image path\n",
    "    subdirs = os.listdir(image_path)\n",
    "    image_fnames = []\n",
    "    for subdir in (subdirs + ['']):  \n",
    "        image_fnames += glob(os.path.join(image_path, subdir, '*.png'))\n",
    "        \n",
    "    image_ids_failing = []\n",
    "    \n",
    "    print('*** Start analysing {} image(s) ***'.format(len(image_fnames)))\n",
    "    \n",
    "    ref_image = np.array(Image.open(ref_path))\n",
    "    norm = StainNormalizationHE(stain_estimation_method='vahadane')\n",
    "    norm.fit_to_reference(ref_image)\n",
    "    ref_stain_matrix = norm.stain_matrix_target_od \n",
    "    ref_max_C = norm.max_c_target \n",
    "    \n",
    "    for image_path in tqdm(image_fnames):\n",
    "        \n",
    "        # a. load image & check if already there \n",
    "        _, image_name = os.path.split(image_path)\n",
    "        image = np.array(Image.open(image_path))\n",
    "\n",
    "        # Compute number of pixels in image and check the label of the image\n",
    "        nr_pixels = image.shape[0] * image.shape[1]\n",
    "        image_label = TUMOR_TYPE_TO_LABEL[image_name.split('_')[2]]\n",
    "\n",
    "        # Get the output file paths of cell graphs, tissue graphs and assignment matrices\n",
    "        cg_out = os.path.join(save_path, 'cell_graphs', split, image_name.replace('.png', '.pt'))\n",
    "        tg_out = os.path.join(save_path, 'tissue_graphs', split, image_name.replace('.png', '.pt'))\n",
    "        assign_out = os.path.join(save_path, 'assignment_matrices', split, image_name.replace('.png', '.pt')) \n",
    "\n",
    "        # If file was not already created or not too big or not too small, then process \n",
    "        if not _exists(cg_out, tg_out, assign_out, overwrite) and _valid_image(nr_pixels):\n",
    "            \n",
    "            print(f'Image size: {image.shape[0], image.shape[1]}')\n",
    "\n",
    "            if plot:\n",
    "                print('Input ROI:')\n",
    "                plt.imshow(image)\n",
    "                plt.show()\n",
    "            \n",
    "            try:\n",
    "                # Read the image as a pathml.core.SlideData class\n",
    "                print('\\nReading image')\n",
    "                wsi = SlideData(image_path, name = image_path, backend = \"openslide\", stain = 'HE')\n",
    "\n",
    "                # Apply our HoverNetNucleusDetectionHE as a pathml.preprocessing.Pipeline over all patches\n",
    "                print('Detecting nuclei')\n",
    "                pipeline = Pipeline([HoverNetNucleusDetectionHE(mask_name='cell', \n",
    "                                                                model_path=hovernet_model_path)])\n",
    "                \n",
    "                # Run the Pipeline \n",
    "                wsi.run(pipeline, overwrite_existing_tiles=True, distributed=False, tile_pad=True, tile_size=PATCH_SIZE)\n",
    "\n",
    "                # Extract the ROI, nuclei instance maps as an np.array from a pathml.core.SlideData object\n",
    "                image, nuclei_map, nuclei_centroid = get_full_instance_map(wsi, patch_size = PATCH_SIZE)\n",
    "\n",
    "                # Use a ResNet-34 to extract the features from each detected cell in the ROI\n",
    "                print('Extracting features from cells')\n",
    "                extractor = DeepPatchFeatureExtractor(patch_size=64, \n",
    "                                            batch_size=64, \n",
    "                                            entity = 'cell',\n",
    "                                            architecture='resnet34', \n",
    "                                            fill_value=255, \n",
    "                                            resize_size=224,\n",
    "                                            threshold=0)\n",
    "                features = extractor.process(image, nuclei_map)\n",
    "\n",
    "                # Build a kNN graph with nodes as cells, node features as ResNet-34 computed features, and edges within\n",
    "                # a threshold of 50\n",
    "                print('Building graphs')\n",
    "                knn_graph_builder = KNNGraphBuilder(k=5, thresh=50, add_loc_feats=True)\n",
    "                cell_graph = knn_graph_builder.process(nuclei_map, features, target = image_label)\n",
    "\n",
    "                # Plot cell graph on ROI image \n",
    "                if plot:\n",
    "                    print('Cell graph on ROI:')\n",
    "                    plot_graph_on_image(cell_graph, image)\n",
    "\n",
    "                # Save the cell graph \n",
    "                torch.save(cell_graph, cg_out)\n",
    "\n",
    "                # Detect tissue using pathml.graph.ColorMergedSuperpixelExtractor class\n",
    "                print('\\nDetecting tissue')\n",
    "                tissue_detector = ColorMergedSuperpixelExtractor(superpixel_size=200,\n",
    "                                                                 compactness=20,\n",
    "                                                                 blur_kernel_size=1,\n",
    "                                                                 threshold=0.05,\n",
    "                                                                 downsampling_factor=4)\n",
    "\n",
    "                superpixels, _ = tissue_detector.process(image)\n",
    "\n",
    "                # Use a ResNet-34 to extract the features from each detected tissue in the ROI\n",
    "                print('Extracting features from tissues')\n",
    "                tissue_feature_extractor = DeepPatchFeatureExtractor(architecture='resnet34',\n",
    "                                                                       patch_size=144,\n",
    "                                                                       entity = 'tissue',\n",
    "                                                                       resize_size=224,\n",
    "                                                                       fill_value=255,\n",
    "                                                                       batch_size=32,\n",
    "                                                                       threshold = 0.25)\n",
    "                features = tissue_feature_extractor.process(image, superpixels)\n",
    "\n",
    "                # Build a RAG with tissues as nodes, node features as ResNet-34 computed features, and edges using the \n",
    "                # RAG algorithm\n",
    "                print('Building graphs')\n",
    "                rag_graph_builder = RAGGraphBuilder(add_loc_feats=True)\n",
    "                tissue_graph = rag_graph_builder.process(superpixels, features, target = image_label)\n",
    "\n",
    "                # Plot tissue graph on ROI image\n",
    "                if plot:\n",
    "                    print('Tissue graph on ROI:')\n",
    "                    plot_graph_on_image(tissue_graph, image)\n",
    "\n",
    "                # Save the tissue graph \n",
    "                torch.save(tissue_graph, tg_out)  \n",
    "\n",
    "                # Build as assignment matrix that maps each cell to the tissue it is a part of \n",
    "                assignment = build_assignment_matrix(nuclei_centroid, superpixels)\n",
    "\n",
    "                # Save the assignment matrix\n",
    "                torch.save(torch.tensor(assignment), assign_out)\n",
    "          \n",
    "            except:\n",
    "                print(f'Failed {image_path}')\n",
    "                image_ids_failing.append(image_path)\n",
    "            \n",
    "    print('\\nOut of {} images, {} successful graph generations.'.format(\n",
    "            len(image_fnames),\n",
    "            len(image_fnames) - len(image_ids_failing)\n",
    "        ))\n",
    "    print('Failing IDs are:', image_ids_failing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9a5cd-4e01-4f4f-b5cd-91f1b204835b",
   "metadata": {},
   "source": [
    "Finally, we write a main function that calls the process function for a specified root and output directory, along with the name of the split (either train, test or validation if using BRACS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a71aa-babc-47e7-a641-d9688962350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(base_path, save_path, split=None):\n",
    "    if split is not None:\n",
    "        root_path = os.path.join(base_path, split)\n",
    "    else:\n",
    "        root_path = base_path\n",
    "    \n",
    "    print(root_path)\n",
    "    \n",
    "    os.makedirs(os.path.join(save_path, 'cell_graphs', split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_path, 'tissue_graphs', split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_path, 'assignment_matrices', split), exist_ok=True)\n",
    "    \n",
    "    process(root_path, save_path, split, plot=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d660e9-b32c-4101-9d64-7fb9cdc14c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing all images\n",
    "base = '../data/'\n",
    "\n",
    "# Output path \n",
    "save_path = '../data/output/'\n",
    "\n",
    "# Start preprocessing\n",
    "main(base, save_path, split='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cfdb1-e016-4d9c-acff-b3eb0f40ce55",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*  Pati, Pushpak, Guillaume Jaume, Antonio Foncubierta-Rodriguez, Florinda Feroce, Anna Maria Anniciello, Giosue Scognamiglio, Nadia Brancati et al. \"Hierarchical graph representations in digital pathology.\" Medical image analysis 75 (2022): 102264.\n",
    "*  Brancati, Nadia, Anna Maria Anniciello, Pushpak Pati, Daniel Riccio, Giosuè Scognamiglio, Guillaume Jaume, Giuseppe De Pietro et al. \"Bracs: A dataset for breast carcinoma subtyping in h&e histology images.\" Database 2022 (2022): baac093."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307dfb9-19cc-4f6b-aa96-7f5f40322e0b",
   "metadata": {},
   "source": [
    "## Session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d903d-01ef-492f-a806-d974a0940c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "print(IPython.sys_info())\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathml_graph_dev",
   "language": "python",
   "name": "pathml_graph_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
