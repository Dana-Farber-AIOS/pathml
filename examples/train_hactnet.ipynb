{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655f37d3-1591-4a7c-8f54-f7fc8148dcfd",
   "metadata": {},
   "source": [
    "# Training a HACTNet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88157b8c-0368-42a4-af82-800b7bab74d7",
   "metadata": {},
   "source": [
    "In this notebook, we will train the HACTNet graph neural network (GNN) model on input cell and tissue graphs using the new `pathml.graph` API.\n",
    "\n",
    "To run the notebook and train the model, you will have to first download the BRACS ROI set from the [BRACS dataset](https://www.bracs.icar.cnr.it/download/). To do so, you will have to sign up and create an account. Next, you will have to construct the cell and tissue graphs using the tutorial in `examples/construct_graphs.ipynb`. Use the output directory specified there as the input to the main function in this tutorial.\n",
    "\n",
    "NOTE: The actual HACTNet model uses HoVer-Net, an ML model, to detect cells. In `examples/construct_graphs.ipynb`, we used a manual method for simplicity. Hence the performance of the model trained in this notebook will be lesser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f682f702-b590-4e3d-8c97-a362411acade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import h5py\n",
    "import warnings\n",
    "import math\n",
    "from skimage.measure import regionprops, label\n",
    "import networkx as nx\n",
    "import traceback\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pathml.datasets import EntityDataset\n",
    "from pathml.ml.utils import get_degree_histogram, get_class_weights\n",
    "from pathml.ml import HACTNet\n",
    "\n",
    "device = \"cuda\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = (\n",
    "    \"/home/jupyter/miniforge3/envs/pathml_cuda/lib/python3.10/site-packages/cv2/../../lib64\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4ee6f-7b17-424d-9cdd-710e36c7341c",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10601c10-d069-481d-b502-f98f76e18e3c",
   "metadata": {},
   "source": [
    "Here we define the main training loop for loading the constructed graphs, initializing and training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00cb474e-0441-4ff0-a495-709d3df3759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hactnet(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    load_histogram=True,\n",
    "    histogram_dir=None,\n",
    "    calc_class_weights=True,\n",
    "):\n",
    "\n",
    "    # Print the lengths of each dataset split\n",
    "    print(f\"Length of training dataset: {len(train_dataset)}\")\n",
    "    print(f\"Length of validation dataset: {len(val_dataset)}\")\n",
    "    print(f\"Length of test dataset: {len(test_dataset)}\")\n",
    "\n",
    "    # Define the torch_geometric.DataLoader object for each dataset split with a batch size of 4\n",
    "    train_batch = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        follow_batch=[\"x_cell\", \"x_tissue\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_batch = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        follow_batch=[\"x_cell\", \"x_tissue\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "    test_batch = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        follow_batch=[\"x_cell\", \"x_tissue\"],\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # The GNN layer we use in this model, PNAConv, requires the computation of a node degree histogram of the\n",
    "    # train dataset. We only need to compute it once. If it is precomputed already, set the load_histogram=True.\n",
    "    # Else, the degree histogram is calculated and saved.\n",
    "    if load_histogram:\n",
    "        histogram_dir = \"./\"\n",
    "        cell_deg = torch.load(os.path.join(histogram_dir, \"cell_degree_norm.pt\"))\n",
    "        tissue_deg = torch.load(os.path.join(histogram_dir, \"tissue_degree_norm.pt\"))\n",
    "    else:\n",
    "        train_batch_hist = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=20,\n",
    "            shuffle=True,\n",
    "            follow_batch=[\"x_cell\", \"x_tissue\"],\n",
    "        )\n",
    "        print(\"Calculating degree histogram for cell graph\")\n",
    "        cell_deg = get_degree_histogram(train_batch_hist, \"edge_index_cell\", \"x_cell\")\n",
    "        print(\"Calculating degree histogram for tissue graph\")\n",
    "        tissue_deg = get_degree_histogram(\n",
    "            train_batch_hist, \"edge_index_tissue\", \"x_tissue\"\n",
    "        )\n",
    "        torch.save(cell_deg, \"cell_degree_norm.pt\")\n",
    "        torch.save(tissue_deg, \"tissue_degree_norm.pt\")\n",
    "\n",
    "    # Since the BRACS dataset has unbalanced data, it is important to calculate the class weights in the training set\n",
    "    # and provide that as an argument to our loss function.\n",
    "    if calc_class_weights:\n",
    "        train_w = get_class_weights(train_batch)\n",
    "        torch.save(torch.tensor(train_w), \"loss_weights_norm.pt\")\n",
    "\n",
    "    # Here we define the keyword arguments for the PNAConv layer in the model for both cell and tissue processing\n",
    "    # layers.\n",
    "    kwargs_pna_cell = {\n",
    "        \"aggregators\": [\"mean\", \"max\", \"min\", \"std\"],\n",
    "        \"scalers\": [\"identity\", \"amplification\", \"attenuation\"],\n",
    "        \"deg\": cell_deg,\n",
    "    }\n",
    "    kwargs_pna_tissue = {\n",
    "        \"aggregators\": [\"mean\", \"max\", \"min\", \"std\"],\n",
    "        \"scalers\": [\"identity\", \"amplification\", \"attenuation\"],\n",
    "        \"deg\": tissue_deg,\n",
    "    }\n",
    "\n",
    "    cell_params = {\n",
    "        \"layer\": \"PNAConv\",\n",
    "        \"in_channels\": 514,\n",
    "        \"hidden_channels\": 64,\n",
    "        \"num_layers\": 3,\n",
    "        \"out_channels\": 64,\n",
    "        \"readout_op\": \"lstm\",\n",
    "        \"readout_type\": \"mean\",\n",
    "        \"kwargs\": kwargs_pna_cell,\n",
    "    }\n",
    "\n",
    "    tissue_params = {\n",
    "        \"layer\": \"PNAConv\",\n",
    "        \"in_channels\": 514,\n",
    "        \"hidden_channels\": 64,\n",
    "        \"num_layers\": 3,\n",
    "        \"out_channels\": 64,\n",
    "        \"readout_op\": \"lstm\",\n",
    "        \"readout_type\": \"mean\",\n",
    "        \"kwargs\": kwargs_pna_tissue,\n",
    "    }\n",
    "\n",
    "    classifier_params = {\n",
    "        \"in_channels\": 128,\n",
    "        \"hidden_channels\": 128,\n",
    "        \"out_channels\": 7,\n",
    "        \"num_layers\": 2,\n",
    "    }\n",
    "\n",
    "    # Initialize the pathml.ml.HACTNet model\n",
    "    model = HACTNet(cell_params, tissue_params, classifier_params)\n",
    "\n",
    "    # Set up optimizer\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    # Learning rate scheduler to reduce LR by factor of 10 each 25 epochs\n",
    "    scheduler = StepLR(opt, step_size=25, gamma=0.1)\n",
    "\n",
    "    # Send the model to GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define number of epochs\n",
    "    n_epochs = 60\n",
    "\n",
    "    # Keep a track of best epoch and metric for saving only the best models\n",
    "    best_epoch = 0\n",
    "    best_metric = 0\n",
    "\n",
    "    # Load the computed class weights if calc_class_weights = True\n",
    "    if calc_class_weights:\n",
    "        loss_weights = torch.load(\"loss_weights.pt\")\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_fn = nn.CrossEntropyLoss(\n",
    "        weight=loss_weights.float().to(device) if calc_class_weights else None\n",
    "    )\n",
    "\n",
    "    # Define the evaluate function to compute metrics for validation and test set to keep track of performance.\n",
    "    # The metrics used are per-class and weighted F1 score.\n",
    "    def evaluate(data_loader):\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(data_loader):\n",
    "                data = data.to(device)\n",
    "                outputs = model(data)\n",
    "                y_true.append(\n",
    "                    torch.argmax(outputs.detach().cpu().softmax(dim=1), dim=-1).numpy()\n",
    "                )\n",
    "                y_pred.append(data.target.cpu().numpy())\n",
    "            y_true = np.array(y_true).ravel()\n",
    "            y_pred = np.array(y_pred).ravel()\n",
    "            per_class = f1_score(y_true, y_pred, average=None)\n",
    "            weighted = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "        print(f\"Per class F1: {per_class}\")\n",
    "        print(f\"Weighted F1: {weighted}\")\n",
    "        return np.append(per_class, weighted)\n",
    "\n",
    "    # Start the training loop\n",
    "    for i in range(n_epochs):\n",
    "        print(f\"\\n>>>>>>>>>>>>>>>>Epoch number {i}>>>>>>>>>>>>>>>>\")\n",
    "        minibatch_train_losses = []\n",
    "\n",
    "        # Put model in training mode\n",
    "        model.train()\n",
    "\n",
    "        print(\"Training\")\n",
    "\n",
    "        for data in tqdm(train_batch):\n",
    "\n",
    "            # Step optimizer and scheduler\n",
    "            opt.step()\n",
    "\n",
    "            # Send the data to the GPU\n",
    "            data = data.to(device)\n",
    "\n",
    "            # Zero out gradient\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, data.target)\n",
    "\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Track loss\n",
    "            minibatch_train_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        print(f\"Loss: {np.array(minibatch_train_losses).ravel().mean()}\")\n",
    "\n",
    "        # Print performance metrics on validation set\n",
    "        print(\"\\nEvaluating on validation\")\n",
    "        val_metrics = evaluate(val_batch)\n",
    "\n",
    "        # Save the model only if it is better than previous checkpoint in validation metrics\n",
    "        if val_metrics[-1] > best_metric:\n",
    "            print(\"Saving checkpoint\")\n",
    "            torch.save(model.state_dict(), \"hact_net.pt\")\n",
    "            best_metric = val_metrics[-1]\n",
    "\n",
    "        # Print performance metrics on test set\n",
    "        print(\"\\nEvaluating on test\")\n",
    "        _ = evaluate(test_batch)\n",
    "\n",
    "        # Step LR scheduler\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd0f6179-b6d6-4ff2-afc8-89548e64e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train, validation and test dataset into the pathml.datasets.EntityDataset class\n",
    "root_dir = \"./data/BRACS_RoI/latest_version/output/\"\n",
    "train_dataset = EntityDataset(\n",
    "    os.path.join(root_dir, \"cell_graphs/train/\"),\n",
    "    os.path.join(root_dir, \"tissue_graphs/train/\"),\n",
    "    os.path.join(root_dir, \"assignment_matrices/train/\"),\n",
    ")\n",
    "val_dataset = EntityDataset(\n",
    "    os.path.join(root_dir, \"cell_graphs/val/\"),\n",
    "    os.path.join(root_dir, \"tissue_graphs/val/\"),\n",
    "    os.path.join(root_dir, \"assignment_matrices/val/\"),\n",
    ")\n",
    "test_dataset = EntityDataset(\n",
    "    os.path.join(root_dir, \"cell_graphs/test/\"),\n",
    "    os.path.join(root_dir, \"tissue_graphs/test/\"),\n",
    "    os.path.join(root_dir, \"assignment_matrices/test/\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f408188-5804-4ac4-9a1e-642c6e5e6d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 3627\n",
      "Length of validation dataset: 311\n",
      "Length of test dataset: 563\n",
      "\n",
      ">>>>>>>>>>>>>>>>Epoch number 0>>>>>>>>>>>>>>>>\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 453/453 [16:33<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.681248664855957\n",
      "\n",
      "Evaluating on validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 38/38 [01:19<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per class F1: [0.14285714 0.23404255 0.         0.         0.         0.27802691\n",
      " 0.71287129]\n",
      "Weighted F1: 0.34292555902950034\n",
      "Saving checkpoint\n",
      "\n",
      "Evaluating on test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 70/70 [01:44<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per class F1: [0.29906542 0.34177215 0.         0.         0.         0.30291262\n",
      " 0.32323232]\n",
      "Weighted F1: 0.30912310035688134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_hactnet(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    load_histogram=True,\n",
    "    calc_class_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67837805-14b5-4d08-8e5f-ffd5afdaf9bb",
   "metadata": {},
   "source": [
    "After training the model for 40-50 epochs, you should see performance similar to the table below, depending on the dataset version you used. \n",
    "\n",
    "\n",
    "| Dataset                  | Weighted F-1 score |\n",
    "|--------------------------|--------------------|\n",
    "| BRACS (Previous version) | 60.14              |\n",
    "| BRACS (Latest Version)   | 55.96              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d6c48-c5be-4dcd-a38e-6277d1fd5956",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*  Pati, Pushpak, Guillaume Jaume, Antonio Foncubierta-Rodriguez, Florinda Feroce, Anna Maria Anniciello, Giosue Scognamiglio, Nadia Brancati et al. \"Hierarchical graph representations in digital pathology.\" Medical image analysis 75 (2022): 102264.\n",
    "*  Brancati, Nadia, Anna Maria Anniciello, Pushpak Pati, Daniel Riccio, Giosuè Scognamiglio, Guillaume Jaume, Giuseppe De Pietro et al. \"Bracs: A dataset for breast carcinoma subtyping in h&e histology images.\" Database 2022 (2022): baac093."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ea74a-da2b-4e92-8d29-cbf7f59792cb",
   "metadata": {},
   "source": [
    "## Session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0c5f9f8-cb8c-4d61-9147-7d82bcb45c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'commit_hash': '8b1204b6c',\n",
      " 'commit_source': 'installation',\n",
      " 'default_encoding': 'utf-8',\n",
      " 'ipython_path': '/home/jupyter/miniforge3/envs/pathml_cuda/lib/python3.10/site-packages/IPython',\n",
      " 'ipython_version': '8.21.0',\n",
      " 'os_name': 'posix',\n",
      " 'platform': 'Linux-4.19.0-26-cloud-amd64-x86_64-with-glibc2.28',\n",
      " 'sys_executable': '/home/jupyter/miniforge3/envs/pathml_cuda/bin/python',\n",
      " 'sys_platform': 'linux',\n",
      " 'sys_version': '3.10.13 | packaged by conda-forge | (main, Dec 23 2023, '\n",
      "                '15:36:39) [GCC 12.3.0]'}\n",
      "torch version: 1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "print(IPython.sys_info())\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pathml_cuda",
   "name": "pytorch-gpu.1-13.m105",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m105"
  },
  "kernelspec": {
   "display_name": "pathml_cuda",
   "language": "python",
   "name": "pathml_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
